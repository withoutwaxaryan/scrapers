{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "'Rajkot',\n",
    "'Anand',\n",
    "'Bangalore',\n",
    "'Chennai',\n",
    "'Ahmedabad',\n",
    "'Noida',\n",
    "'Faridabad',\n",
    "'Mumbai',\n",
    "'Hyderabad',\n",
    "'Navi-Mumbai',\n",
    "'Gurgaon',\n",
    "'Greater-Noida',\n",
    "'Pune',\n",
    "'Kolkata',\n",
    "'Thane',\n",
    "'Ghaziabad',\n",
    "'New-Delhi',\n",
    "'Agra',\n",
    "'Durgapur',\n",
    "'Kochi',\n",
    "'Palghar',\n",
    "'Trichy',\n",
    "'Allahabad',\n",
    "'Goa',\n",
    "'Kottayam',\n",
    "'Patna',\n",
    "'Udaipur',\n",
    "'Aurangabad',\n",
    "'Gorakhpur',\n",
    "'Kozhikode',\n",
    "'Rajahmundry',\n",
    "'Udupi',\n",
    "'Bhiwadi',\n",
    "'Guntur',\n",
    "'Lucknow',\n",
    "'Ranchi',\n",
    "'Vadodara',\n",
    "'Bhopal',\n",
    "'Guwahati',\n",
    "'Madurai',\n",
    "'Raipur',\n",
    "'Vapi',\n",
    "'Bhubaneswar',\n",
    "'Haridwar',\n",
    "'Mangalore',\n",
    "'Salem',\n",
    "'Varanasi',\n",
    "'Bokaro-Steel-City',\n",
    "'Indore',\n",
    "'Gwalior',\n",
    "'Sonipat',\n",
    "'Vijayawada',\n",
    "'Chandigarh',\n",
    "'Jaipur',\n",
    "'Mysore',\n",
    "'Surat',\n",
    "'Visakhapatnam',\n",
    "'Coimbatore',\n",
    "'Jamshedpur',\n",
    "'Nagpur',\n",
    "'Thrissur',\n",
    "'Ahmadnagar',\n",
    "'Dehradun',\n",
    "'Jodhpur',\n",
    "'Nashik',\n",
    "'Tirupati',\n",
    "'Kanpur',\n",
    "'Navsari',\n",
    "'Trivandrum',]\n",
    "cities = sorted(cities)\n",
    "print(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturing all information in a list made of dictionary items\n",
    "information = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_city_link(city):\n",
    "    city = city.lower()\n",
    "    link = \"https://www.magicbricks.com/pg-in-\" + city + \"-pppfr/page-1\"\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_page_link(city, page):\n",
    "    city = city.lower()\n",
    "    link = \"https://www.magicbricks.com/pg-in-\" + city + \"-pppfr/page-\" + str(page)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_pages(link):\n",
    "    html_text = requests.get(link).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    number_of_pages = soup.find_all(\"a\", class_ = \"act\")\n",
    "    if number_of_pages:\n",
    "        last_page = number_of_pages[-1]\n",
    "        last_page = last_page.get('href')\n",
    "        last_page = int(last_page.split('-')[-1])\n",
    "    else:\n",
    "        last_page = 1\n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_page_scraper(city, link):\n",
    "    html_text = requests.get(link).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    name1 = soup.find(\"div\", class_=\"pg-detail__header__info__titleblock__pgname\")\n",
    "    if name1:\n",
    "        name= name1.text\n",
    "        name = \" \".join(name.split())\n",
    "    else:\n",
    "        name = \"\"\n",
    "#     print(name)\n",
    "    posted_by1 = soup.find(\"span\", class_=\"pg-detail__header__info__actionblock__posted--name\")\n",
    "    if posted_by1:\n",
    "        posted_by = posted_by1.text\n",
    "        posted_by = \" \".join(posted_by.split())\n",
    "    else:\n",
    "        posted_by = \"\"\n",
    "#     print((posted_by).strip())\n",
    "#     property_id = soup.find(\"div\", class_ = \"pg-detail__breadcrumb__propid\").text\n",
    "#     print(property_id)\n",
    "    location1 = soup.find(\"a\", class_ = \"pg-detail__header__info__titleblock__pgloc--text _j-Load_Location_Map\")\n",
    "    if location1:\n",
    "        location = location1.get('data-address')\n",
    "    else:\n",
    "        location = \"\"\n",
    "#     print(location)\n",
    "    price1 = soup.find(\"div\", \"pg-detail__header__info__priceblock--price\")\n",
    "    if price1:\n",
    "        price = price1.text\n",
    "    else:\n",
    "        price = \"\"\n",
    "#     print(price)\n",
    "    occupancy_type1 = soup.find(\"span\", class_ = \"typelist\")\n",
    "    if occupancy_type1:\n",
    "        occupancy_type = occupancy_type1.text.strip()\n",
    "    else:\n",
    "        occupancy_type = \"\"\n",
    "#     print(occupancy_type)  # remove extra lines and commas from this\n",
    "    description = soup.find(\"div\", attrs={\"id\": \"pg_description_more\"})\n",
    "    descriptiony = soup.find(\"div\", attrs={\"id\": \"pg_description_less\"})\n",
    "    if description:\n",
    "        description1 = description.p.text[:-5].strip()\n",
    "#         print(description1)\n",
    "    elif descriptiony:\n",
    "        description1 = descriptiony.p.text[:-5].strip()\n",
    "    else:\n",
    "        description1 = \"\"\n",
    "#         print(description1)\n",
    "    resulto = []\n",
    "    property_details = soup.find_all(\"li\", class_ = \"pg-prop-details__info__grid--item\")\n",
    "    if property_details:\n",
    "        for detail in property_details:\n",
    "            detail1 = detail.find(\"div\", class_ = \"pg-prop-details__info__grid--label\")\n",
    "            detail1 = \" \".join(detail1.text.split())\n",
    "            detail2 = detail.find(\"div\", class_ = \"pg-prop-details__info__grid--value\")\n",
    "            detail2 = \" \".join(detail2.text.split())\n",
    "    #         print(detail1)   \n",
    "    #         print(detail2)\n",
    "            result1 = \"\\n\".join([\" \".join(elem) for elem in zip(detail1.split('\\n'), detail2.split('\\n'))])\n",
    "    #         print(result1)\n",
    "            resulto.append(result1)\n",
    "    else:\n",
    "        property_details = \"\"\n",
    "    resulty = []\n",
    "    amenities = soup.find_all(\"div\", class_ = \"pg-detail__service-list--item__info\")\n",
    "    if amenities:\n",
    "        for amenity in amenities:\n",
    "    #         print(amenity.text)\n",
    "            amenity1 = amenity.find(\"div\", class_ = \"pg-detail__service-list--item--label\")\n",
    "            amenity1 = \" \".join(amenity1.text.split())\n",
    "            amenity2 = amenity.find(\"div\", class_ = \"pg-detail__service-list--item--hint\")\n",
    "            amenity3 = amenity.find(\"div\", class_ = \"pg-detail__rules--allow yes\")\n",
    "            amenity4 = amenity.find(\"div\", class_ = \"pg-detail__rules--allow no\")\n",
    "    #         print(amenity1)\n",
    "            result2 = amenity1\n",
    "            if amenity2:\n",
    "                amenity2 = \" \".join(amenity2.text.split())\n",
    "    #             print(amenity2)\n",
    "                result2 = amenity1 + \" \" + amenity2\n",
    "            if amenity3:\n",
    "                result2 = amenity1 + \" yes\"\n",
    "            if amenity4:\n",
    "                result2 = amenity1 + \" no\"\n",
    "    #         print(result2)\n",
    "            resulty.append(result2)\n",
    "    else:\n",
    "        amenities = \"\"\n",
    "    imagery = []\n",
    "    images1 = soup.find(\"div\", class_ = \"pg-swiper-main no-gradient pg-prop-details__pics__list--item__slider__wrapper\")\n",
    "    if images1:\n",
    "        images = images1.find_all(\"img\", class_ = \"swiper-lazy\")\n",
    "        for image in images:\n",
    "            image = image.get('data-src')\n",
    "            image = image.replace('Photo_h400_w540', 'full_photo')\n",
    "            image = image.replace('_400_540', '')\n",
    "            imagery.append(image)\n",
    "    #         print(image)\n",
    "    room_item = {\n",
    "            \"city\": city,\n",
    "            \"living_type\": \"pg\",\n",
    "            \"name\": name,\n",
    "            \"owner\": posted_by,\n",
    "            \"link\": link,\n",
    "            \"occupancy_type\": occupancy_type,\n",
    "            \"location\": location,\n",
    "            \"rent_per_month\": price,\n",
    "            \"images_links\": imagery,\n",
    "            \"description\": description1,\n",
    "            \"details\": resulto,\n",
    "            \"amenities\": resulty,\n",
    "        }\n",
    "    information.append(room_item)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = \"https://www.magicbricks.com/propertyDetail/cobeds-pg-sangam-vihar-in-new-delhi&pgid=4d42313235343833677261646532\"\n",
    "# link = \"https://www.magicbricks.com/propertyDetail/paying-guest-pg-kamla-nagar-in-agra&pgid=4d423832393937677261646532\"\n",
    "# single_page_scraper(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = \"https://www.magicbricks.com/pg-in-new-delhi-pppfr/page-2\"\n",
    "# number_of_pages(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bangalore, chennai, \n",
    "# cities = cities[:10]\n",
    "# print(cities)\n",
    "cities = ['Ahmadnagar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    print(\"city: \" + city)\n",
    "    city_link = create_city_link(city)\n",
    "#     print(city_link)\n",
    "    all_pages = number_of_pages(city_link)\n",
    "    print(\"number of pages: \" + str(all_pages))\n",
    "    for page in range(1, all_pages + 1):\n",
    "        link = create_page_link(city, page)\n",
    "#             print(link)\n",
    "        data = requests.get(link).text\n",
    "        soupy = BeautifulSoup(data, 'lxml')\n",
    "        places1 = soupy.find_all('div', class_ = \"m-srp-card clearfix\")\n",
    "        places2 = soupy.find_all('div', class_ = \"m-srp-card verified-border clearfix\")\n",
    "        places3 = soupy.find_all('div', class_ = \"m-srp-card  clearfix\")\n",
    "        if places1:\n",
    "            for place in places1:\n",
    "#                 print(place)\n",
    "                place_link = place.get('onclick')[26:-12]\n",
    "#                     print(place_link)\n",
    "                try:\n",
    "                    single_page_scraper(city, place_link)\n",
    "#                     time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(place_link)\n",
    "                    print(e)\n",
    "            time.sleep(0.5)\n",
    "        if places2:\n",
    "            for place in places2:\n",
    "#                 print(place)\n",
    "                place_link = place.get('onclick')[26:-12]\n",
    "#                     print(place_link)\n",
    "                try:\n",
    "                    single_page_scraper(city, place_link)\n",
    "#                     time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(place_link)\n",
    "                    print(e)\n",
    "            time.sleep(0.5)\n",
    "        if places3:\n",
    "            for place in places3:\n",
    "#                 print(place)\n",
    "                place_link = place.get('onclick')[26:-12]\n",
    "#                 print(place_link)\n",
    "                try:\n",
    "                    single_page_scraper(city, place_link)\n",
    "#                     time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(place_link)\n",
    "                    print(e)\n",
    "            time.sleep(0.5)\n",
    "    print(len(information))\n",
    "    print(city + \" done.\")\n",
    "#     break  # To check for one city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surat & vadodara are duplicated, remove manually\n",
    "print(len(information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Magicbricks_pg_data.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
